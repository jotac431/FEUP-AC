{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AC Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "teams_data = pd.read_csv(\"Dataset/teams.csv\")\n",
    "teams_correlation_matrix = teams_data.corr(numeric_only=True)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = abs(teams_correlation_matrix) < 0.5\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(40, 30))\n",
    "sns.heatmap(teams_correlation_matrix, annot=True, cmap='coolwarm', mask=mask)\n",
    "\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add points per game (PPG) column\n",
    "teams_data['PPG'] = teams_data['o_pts'] / teams_data['GP']\n",
    "\n",
    "ppg_per_team = teams_data.groupby(['name', 'year'])['PPG'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "colors = plt.cm.tab20([i/len(ppg_per_team['name'].unique()) for i in range(len(ppg_per_team['name'].unique()))])\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=colors)\n",
    "\n",
    "for team in ppg_per_team['name'].unique():\n",
    "    team_data = ppg_per_team[ppg_per_team['name'] == team]\n",
    "    plt.plot(team_data['year'], team_data['PPG'], label=team)\n",
    "\n",
    "plt.title('Points Per Game (PPG) Evolution per Year for Each Team')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('PPG')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read players_data\n",
    "players_teams_data = pd.read_csv(\"Dataset/players_teams.csv\")\n",
    "players_info_data = pd.read_csv(\"Dataset/players.csv\")\n",
    "\n",
    "# Merge players_teams_data and players_info_data\n",
    "players_data = pd.merge(players_teams_data, players_info_data, left_on='playerID', right_on='bioID', how='left')\n",
    "\n",
    "# Drop rows that are not needed\n",
    "players_data = players_data[players_data.pos.notnull()]\n",
    "players_data.drop(players_data[players_data['weight'] < 60].index, inplace = True)\n",
    "players_data.drop(players_data[players_data['height'] < 50 ].index, inplace = True)\n",
    "players_data.drop(columns=[\"firstseason\",\"lastseason\",\"deathDate\"],inplace=True)\n",
    "players_data.reset_index()\n",
    "players_data.describe()\n",
    "\n",
    "players_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read awards data\n",
    "awards_players_data = pd.read_csv(\"Dataset/awards_players.csv\")\n",
    "\n",
    "# Add awards column to players_data\n",
    "players_data['awards'] = 0\n",
    "\n",
    "for idx, player in players_data.iterrows():\n",
    "    awards_until_target_season = awards_players_data[(awards_players_data['playerID'] == player['playerID']) & (awards_players_data['year'] < player['year'])]\n",
    "    if(not awards_until_target_season.empty):\n",
    "        players_data.loc[[idx], 'awards'] =  len(awards_until_target_season)\n",
    "\n",
    "players_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "# Convert birthDate to age\n",
    "def age(born, year): \n",
    "    born = datetime.strptime(born, \"%Y-%m-%d\").date()\n",
    "    today = date.today()\n",
    "    return (today.year-(25-year)) - born.year - ((today.month, today.day) < (born.month, born.day)) \n",
    "\n",
    "# Add variables to teams data\n",
    "for idx, team in teams_data.iterrows():\n",
    "        # Add average player height column to teams_data\n",
    "        team_players_until_target_season = players_data[(players_data[\"year\"] == team[\"year\"] )&( players_data[\"tmID\"] == team[\"tmID\"])]\n",
    "        team_players_avg_height = team_players_until_target_season[\"height\"].mean()\n",
    "        teams_data.loc[[idx], 'avg_height'] = team_players_avg_height\n",
    "\n",
    "        # Add average player weight column to teams_data\n",
    "        team_players_avg_weight = team_players_until_target_season[\"weight\"].mean()\n",
    "        teams_data.loc[[idx], 'avg_weight'] = team_players_avg_weight\n",
    "\n",
    "        # Add average player age column to teams_data\n",
    "        players_ages = team_players_until_target_season[\"birthDate\"].apply(age, args=(team[\"year\"],))\n",
    "        team_players_avg_age = players_ages.mean()\n",
    "        teams_data.loc[[idx], 'avg_age'] = team_players_avg_age\n",
    "\n",
    "        # Add awards column to teams_data\n",
    "        team_awards_until_target_season = players_data[(players_data[\"year\"] == team[\"year\"] )&( players_data[\"tmID\"] == team[\"tmID\"])]\n",
    "        team_awards_num = team_awards_until_target_season[\"awards\"].sum()\n",
    "        teams_data.loc[[idx], 'awards'] = team_awards_num\n",
    "\n",
    "teams_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_data.drop(columns=['rank', 'firstRound', 'semis', 'finals'],inplace=True)\n",
    "\n",
    "char_map = {'N': 0, 'Y': 1, 'L': 0, 'W': 1}\n",
    "teams_data['playoff'] = teams_data['playoff'].map(char_map)\n",
    "teams_data['playoff'] = teams_data['playoff'].fillna(-1)\n",
    "\n",
    "tmIds = teams_data['tmID'].unique()\n",
    "for i in range (len(tmIds)):\n",
    "    teams_data['tmID'] = teams_data['tmID'].replace(tmIds[i],i)\n",
    "\n",
    "confids = teams_data['confID'].unique()\n",
    "for i in range (len(confids)):\n",
    "    teams_data['confID'] = teams_data['confID'].replace(confids[i],i)\n",
    "\n",
    "arenas = teams_data['arena'].unique()\n",
    "for i in range (len(arenas)):\n",
    "    teams_data['arena'] = teams_data['arena'].replace(arenas[i],i)\n",
    "\n",
    "tmNames = teams_data['name'].unique()\n",
    "for i in range (len(tmNames)):\n",
    "    teams_data['name'] = teams_data['name'].replace(tmNames[i],i)\n",
    "\n",
    "teams_test = teams_data[teams_data['year'] == 9]\n",
    "teams_data = teams_data[teams_data['year'] < 9]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = teams_data.drop(columns=['playoff']), teams_test.drop(columns=['playoff']), teams_data['playoff'], teams_test['playoff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "class AbstractModel(ABC):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, algorithm):\n",
    "        self.X_train = X_train.values\n",
    "        self.X_test = X_test.values\n",
    "        self.y_train = y_train.values\n",
    "        self.y_test = y_test.values\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, clf):\n",
    "        return clf.predict(self.X_test)\n",
    "\n",
    "    def evaluate(self, pred):\n",
    "        return {\n",
    "            'accuracy': accuracy_score(self.y_test, pred),\n",
    "            'f1_score': f1_score(self.y_test, pred)\n",
    "        }\n",
    "\n",
    "    def confusion_matrix(self, pred):\n",
    "        cm = confusion_matrix(self.y_test, pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])\n",
    "        disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "class DecisionTreeModel(AbstractModel):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        super().__init__(X_train, X_test, y_train, y_test, tree.DecisionTreeClassifier())\n",
    "\n",
    "    def train(self):\n",
    "        clf = self.algorithm.fit(self.X_train, self.y_train)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "class SVMModel(AbstractModel):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        super().__init__(X_train, X_test, y_train, y_test, svm.SVC())\n",
    "\n",
    "    def train(self):\n",
    "        clf = self.algorithm.fit(self.X_train, self.y_train)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "class NaiveBayesModel(AbstractModel):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        super().__init__(X_train, X_test, y_train, y_test, GaussianNB())\n",
    "\n",
    "    def train(self):\n",
    "        clf = self.algorithm.fit(self.X_train, self.y_train)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "class NeuralNetworkModel(AbstractModel):\n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        super().__init__(X_train, X_test, y_train, y_test, MLPClassifier())\n",
    "\n",
    "    def train(self):\n",
    "        clf = self.algorithm.fit(self.X_train, self.y_train)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Decision Tree Model\n",
    "decisionTreeModel = DecisionTreeModel(X_train, X_test, y_train, y_test)\n",
    "clf = decisionTreeModel.train()\n",
    "pred = decisionTreeModel.predict(clf)\n",
    "print(decisionTreeModel.evaluate(pred))\n",
    "decisionTreeModel.confusion_matrix(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVM Model\n",
    "svmModel = SVMModel(X_train, X_test, y_train, y_test)\n",
    "clf = svmModel.train()\n",
    "pred = svmModel.predict(clf)\n",
    "print(svmModel.evaluate(pred))\n",
    "svmModel.confusion_matrix(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Naive Bayes Model\n",
    "naiveBayesModel = NaiveBayesModel(X_train, X_test, y_train, y_test)\n",
    "clf = naiveBayesModel.train()\n",
    "pred = naiveBayesModel.predict(clf)\n",
    "print(naiveBayesModel.evaluate(pred))\n",
    "naiveBayesModel.confusion_matrix(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Neural Network Model\n",
    "neuralNetworkModel = NeuralNetworkModel(X_train, X_test, y_train, y_test)\n",
    "clf = neuralNetworkModel.train()\n",
    "pred = neuralNetworkModel.predict(clf)\n",
    "print(neuralNetworkModel.evaluate(pred))\n",
    "neuralNetworkModel.confusion_matrix(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
